# LUTAGU MVP - Rust é·ç§»å„ªåŒ–æ–¹æ¡ˆ

**ç‰ˆæœ¬**: v1.0
**å»ºç«‹æ—¥æœŸ**: 2026-01-21
**ç›®æ¨™**: å°‡æ•ˆèƒ½é—œéµè·¯å¾‘å¾ TypeScript é·ç§»è‡³ Rust

---

## ğŸ“‹ åŸ·è¡Œæ‘˜è¦

### ç¾æ³åˆ†æ
- âœ… **å·²å®Œæˆ**: L2 å³æ™‚ç‹€æ…‹æœå‹™ã€L4 è·¯ç·šè¦åŠƒæœå‹™
- ğŸ¯ **å¾…å„ªåŒ–**: ETL Pipeline (30åˆ†é˜ â†’ 3åˆ†é˜)ã€å‘é‡æœå°‹ (100ms â†’ 5ms)ã€ODPT Client æ¨™æº–åŒ–

### å„ªå…ˆé †åº

| å„ªå…ˆç´š | çµ„ä»¶ | é æœŸæ•ˆèƒ½æå‡ | å¯¦ä½œé›£åº¦ | ROI |
|--------|------|-------------|---------|-----|
| â­â­â­â­â­ | ETL Pipeline | **10-20å€** | ä¸­ | æ¥µé«˜ |
| â­â­â­â­ | ODPT Client | **3-5å€** | ä½ | é«˜ |
| â­â­â­â­ | å‘é‡æœå°‹ | **20å€** | ä¸­ | é«˜ |
| â­â­â­ | æ±ºç­–å¼•æ“ | **5-10å€** | é«˜ | ä¸­ |
| â­â­â­ | çµ±ä¸€å¿«å– | **2-3å€** | ä¸­ | ä¸­ |

---

## ğŸ¯ Phase 1: ETL Pipeline Rust åŒ– (ç«‹å³åŸ·è¡Œ)

### 1.1 ç¾æ³å•é¡Œ

**æª”æ¡ˆ**: `scripts/l3_fill_toilets.ts` (197 lines)

**æ•ˆèƒ½ç“¶é ¸**:
```typescript
// åºåˆ—è™•ç†,æ¯å€‹è»Šç«™å»¶é² 1.5 ç§’
for (const station of stations) {
    const elements = await fetchOverpassToilets(coords.lat, coords.lon);
    // ...
    await sleep(1500);  // â† ä¸»è¦ç“¶é ¸
}
```

**æ™‚é–“æˆæœ¬**:
- 500 å€‹è»Šç«™ Ã— 1.5 ç§’ = **12.5 åˆ†é˜**
- JSON åºåˆ—åŒ–/ååºåˆ—åŒ–é¡å¤–é–‹éŠ·
- å–®åŸ·è¡Œç·’è™•ç†,ç„¡æ³•åˆ©ç”¨å¤šæ ¸å¿ƒ

### 1.2 Rust è§£æ±ºæ–¹æ¡ˆ

#### ç›®éŒ„çµæ§‹
```
services/etl-pipeline-rs/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â””â”€â”€ src/
    â”œâ”€â”€ main.rs              # CLI entry point
    â”œâ”€â”€ cli.rs               # Command definitions
    â”œâ”€â”€ modules/
    â”‚   â”œâ”€â”€ mod.rs
    â”‚   â”œâ”€â”€ l3_toilets.rs    # Toilets scraper
    â”‚   â”œâ”€â”€ l3_osm.rs        # Generic OSM scraper
    â”‚   â””â”€â”€ odpt_client.rs   # Reusable ODPT client
    â”œâ”€â”€ db/
    â”‚   â”œâ”€â”€ mod.rs
    â”‚   â””â”€â”€ supabase.rs      # Database operations
    â””â”€â”€ utils/
        â”œâ”€â”€ mod.rs
        â”œâ”€â”€ http.rs          # HTTP client with retry
        â””â”€â”€ rate_limit.rs    # Rate limiter
```

#### å¯¦ä½œç¯„ä¾‹æª”æ¡ˆ

**ä½ç½®**: `services/etl-pipeline-rs/Cargo.toml`
```toml
[package]
name = "etl-pipeline-rs"
version = "0.1.0"
edition = "2021"

[[bin]]
name = "lutagu-etl"
path = "src/main.rs"

[dependencies]
tokio = { version = "1.38", features = ["full"] }
reqwest = { version = "0.12", features = ["json", "rustls-tls"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
sqlx = { version = "0.8", features = ["runtime-tokio-rustls", "postgres", "chrono", "json"] }
clap = { version = "4.5", features = ["derive"] }
anyhow = "1.0"
tracing = "0.1"
tracing-subscriber = "0.3"
governor = "0.6"  # Rate limiting
futures = "0.3"
rayon = "1.10"    # Parallel processing

[dev-dependencies]
tokio-test = "0.4"
```

**ä½ç½®**: `services/etl-pipeline-rs/src/main.rs`
```rust
use clap::{Parser, Subcommand};
use tracing_subscriber;

mod cli;
mod modules;
mod db;
mod utils;

#[derive(Parser)]
#[command(name = "lutagu-etl")]
#[command(about = "LUTAGU ETL Pipeline - Rust Edition", long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Fill L3 toilet facilities from OpenStreetMap
    FillToilets {
        /// Radius in meters
        #[arg(short, long, default_value_t = 150)]
        radius: u32,

        /// Delay between requests (ms)
        #[arg(short, long, default_value_t = 100)]
        delay: u64,

        /// Number of concurrent workers
        #[arg(short, long, default_value_t = 10)]
        workers: usize,
    },

    /// Fill L3 facilities from OSM (generic)
    FillOsm {
        /// OSM amenity type (e.g., "cafe", "restaurant")
        #[arg(short, long)]
        amenity: String,

        #[arg(short, long, default_value_t = 150)]
        radius: u32,

        #[arg(short, long, default_value_t = 10)]
        workers: usize,
    },

    /// Fetch ODPT station data
    FetchOdpt {
        /// Operators (comma-separated)
        #[arg(short, long)]
        operators: String,

        /// Output JSON file path
        #[arg(short, long)]
        output: Option<String>,
    },
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // Initialize logging
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .init();

    let cli = Cli::parse();

    match cli.command {
        Commands::FillToilets { radius, delay, workers } => {
            modules::l3_toilets::run(radius, delay, workers).await?;
        }
        Commands::FillOsm { amenity, radius, workers } => {
            modules::l3_osm::run(&amenity, radius, workers).await?;
        }
        Commands::FetchOdpt { operators, output } => {
            modules::odpt_client::fetch_stations(&operators, output.as_deref()).await?;
        }
    }

    Ok(())
}
```

**ä½ç½®**: `services/etl-pipeline-rs/src/modules/l3_toilets.rs`
```rust
use anyhow::Result;
use futures::stream::{self, StreamExt};
use serde::{Deserialize, Serialize};
use serde_json::json;
use std::time::Duration;
use tracing::{info, warn};

use crate::db::supabase::SupabaseClient;
use crate::utils::http::HttpClient;
use crate::utils::rate_limit::RateLimiter;

const OVERPASS_URL: &str = "https://overpass-api.de/api/interpreter";

#[derive(Debug, Deserialize)]
struct OverpassResponse {
    elements: Vec<OverpassElement>,
}

#[derive(Debug, Deserialize)]
struct OverpassElement {
    #[serde(rename = "type")]
    element_type: String,
    id: i64,
    lat: Option<f64>,
    lon: Option<f64>,
    center: Option<LatLon>,
    tags: Option<serde_json::Value>,
}

#[derive(Debug, Deserialize)]
struct LatLon {
    lat: f64,
    lon: f64,
}

#[derive(Debug, Deserialize)]
struct StationRecord {
    id: String,
    coordinates: serde_json::Value,
    name: serde_json::Value,
}

#[derive(Debug, Serialize)]
struct FacilityInsert {
    station_id: String,
    #[serde(rename = "type")]
    facility_type: String,
    name_i18n: serde_json::Value,
    location_coords: Option<String>,
    attributes: serde_json::Value,
    source_url: String,
    updated_at: String,
}

pub async fn run(radius: u32, delay_ms: u64, workers: usize) -> Result<()> {
    info!("ğŸš€ Starting L3 Toilets ETL (Rust)");
    info!("   Radius: {}m, Delay: {}ms, Workers: {}", radius, delay_ms, workers);

    let db = SupabaseClient::from_env()?;
    let http = HttpClient::new(Duration::from_secs(30));
    let rate_limiter = RateLimiter::new(workers, Duration::from_millis(delay_ms));

    // Fetch active stations
    let stations: Vec<StationRecord> = db
        .query("SELECT id, coordinates, name FROM nodes WHERE is_active = true")
        .await?;

    info!("ğŸ“ Found {} active stations", stations.len());

    let total = stations.len();
    let mut processed = 0;
    let mut total_inserted = 0;

    // Parallel processing with Tokio streams
    let results: Vec<_> = stream::iter(stations)
        .map(|station| {
            let http = http.clone();
            let db = db.clone();
            let rate_limiter = rate_limiter.clone();

            async move {
                // Rate limiting
                rate_limiter.wait().await;

                let coords = extract_coords(&station)?;
                let elements = fetch_overpass_toilets(&http, coords.0, coords.1, radius).await?;

                if elements.is_empty() {
                    return Ok::<_, anyhow::Error>(0);
                }

                let facilities: Vec<FacilityInsert> = elements
                    .into_iter()
                    .map(|el| transform_toilet(el, &station.id))
                    .collect();

                // Check existing OSM IDs
                let existing_ids = db.get_existing_osm_ids(&station.id, "toilet").await?;
                let new_facilities: Vec<_> = facilities
                    .into_iter()
                    .filter(|f| {
                        if let Some(attrs) = f.attributes.as_object() {
                            if let Some(osm_id) = attrs.get("osm_id").and_then(|v| v.as_i64()) {
                                return !existing_ids.contains(&osm_id);
                            }
                        }
                        true
                    })
                    .collect();

                if !new_facilities.is_empty() {
                    db.insert_facilities(&new_facilities).await?;
                    info!("  âœ… {} - Added {} new toilets", station.id, new_facilities.len());
                    Ok(new_facilities.len())
                } else {
                    info!("  âœ¨ {} - All toilets already exist", station.id);
                    Ok(0)
                }
            }
        })
        .buffer_unordered(workers) // Concurrent execution
        .collect()
        .await;

    // Aggregate results
    for result in results {
        processed += 1;
        match result {
            Ok(count) => total_inserted += count,
            Err(e) => warn!("Error processing station: {}", e),
        }
    }

    info!("============================================");
    info!("ğŸ“Š Toilet Supplement Complete!");
    info!("   Processed: {}/{}", processed, total);
    info!("   Total Toilets Added: {}", total_inserted);

    Ok(())
}

async fn fetch_overpass_toilets(
    http: &HttpClient,
    lat: f64,
    lon: f64,
    radius: u32,
) -> Result<Vec<OverpassElement>> {
    let query = format!(
        r#"
        [out:json][timeout:25];
        (
            node["amenity"="toilets"](around:{},{},{});
            way["amenity"="toilets"](around:{},{},{});
        );
        out center tags;
        "#,
        radius, lat, lon, radius, lat, lon
    );

    let body = format!("data={}", urlencoding::encode(&query));

    let response = http
        .post(OVERPASS_URL)
        .header("Content-Type", "application/x-www-form-urlencoded")
        .body(body)
        .send()
        .await?;

    if response.status() == 429 {
        warn!("Overpass 429 (Too Many Requests)");
        tokio::time::sleep(Duration::from_secs(10)).await;
        return Ok(vec![]);
    }

    if !response.status().is_success() {
        warn!("Overpass returned {}", response.status());
        return Ok(vec![]);
    }

    let data: OverpassResponse = response.json().await?;
    Ok(data.elements)
}

fn extract_coords(station: &StationRecord) -> Result<(f64, f64)> {
    let coords = station.coordinates.as_object()
        .ok_or_else(|| anyhow::anyhow!("Invalid coordinates format"))?;

    let coords_array = coords.get("coordinates")
        .and_then(|c| c.as_array())
        .ok_or_else(|| anyhow::anyhow!("Missing coordinates array"))?;

    let lon = coords_array.get(0).and_then(|v| v.as_f64()).unwrap_or(0.0);
    let lat = coords_array.get(1).and_then(|v| v.as_f64()).unwrap_or(0.0);

    Ok((lat, lon))
}

fn transform_toilet(element: OverpassElement, station_id: &str) -> FacilityInsert {
    let tags = element.tags.as_ref();

    let name_en = tags
        .and_then(|t| t.get("name:en").or(t.get("name")))
        .and_then(|v| v.as_str())
        .unwrap_or("Public Restroom");

    let name_ja = tags
        .and_then(|t| t.get("name:ja").or(t.get("name")))
        .and_then(|v| v.as_str())
        .unwrap_or("å…¬è¡†ãƒˆã‚¤ãƒ¬");

    let attributes = json!({
        "osm_id": element.id,
        "source": "OpenStreetMap",
        "fee": tags.and_then(|t| t.get("fee")).and_then(|v| v.as_str()) == Some("yes"),
        "wheelchair": tags.and_then(|t| t.get("wheelchair"))
            .and_then(|v| v.as_str())
            .map(|s| s == "yes" || s == "designated")
            .unwrap_or(false),
        "changing_table": tags.and_then(|t| t.get("changing_table").or(t.get("diaper")))
            .and_then(|v| v.as_str()) == Some("yes"),
        "unisex": tags.and_then(|t| t.get("unisex")).and_then(|v| v.as_str()) == Some("yes"),
        "operator": tags.and_then(|t| t.get("operator")).and_then(|v| v.as_str()),
    });

    let (lat, lon) = if let Some(center) = &element.center {
        (center.lat, center.lon)
    } else {
        (element.lat.unwrap_or(0.0), element.lon.unwrap_or(0.0))
    };

    FacilityInsert {
        station_id: station_id.to_string(),
        facility_type: "toilet".to_string(),
        name_i18n: json!({
            "en": name_en,
            "ja": name_ja,
        }),
        location_coords: if lat != 0.0 && lon != 0.0 {
            Some(format!("POINT({} {})", lon, lat))
        } else {
            None
        },
        attributes,
        source_url: format!(
            "https://www.openstreetmap.org/{}/{}",
            element.element_type, element.id
        ),
        updated_at: chrono::Utc::now().to_rfc3339(),
    }
}
```

**ä½ç½®**: `services/etl-pipeline-rs/src/db/supabase.rs`
```rust
use anyhow::Result;
use serde::de::DeserializeOwned;
use serde::Serialize;
use sqlx::{PgPool, postgres::PgPoolOptions, Row};
use std::env;

#[derive(Clone)]
pub struct SupabaseClient {
    pool: PgPool,
}

impl SupabaseClient {
    pub fn from_env() -> Result<Self> {
        let database_url = env::var("DATABASE_URL")
            .expect("DATABASE_URL must be set");

        let pool = PgPoolOptions::new()
            .max_connections(20)
            .connect_lazy(&database_url)?;

        Ok(Self { pool })
    }

    pub async fn query<T: DeserializeOwned>(&self, sql: &str) -> Result<Vec<T>> {
        let rows = sqlx::query(sql)
            .fetch_all(&self.pool)
            .await?;

        let results: Vec<T> = rows
            .into_iter()
            .map(|row| {
                let json_str = serde_json::to_string(&row)?;
                serde_json::from_str(&json_str)
            })
            .collect::<Result<Vec<_>, _>>()?;

        Ok(results)
    }

    pub async fn get_existing_osm_ids(&self, station_id: &str, facility_type: &str) -> Result<Vec<i64>> {
        let rows = sqlx::query(
            r#"
            SELECT attributes->>'osm_id' as osm_id
            FROM l3_facilities
            WHERE station_id = $1 AND type = $2
            "#
        )
        .bind(station_id)
        .bind(facility_type)
        .fetch_all(&self.pool)
        .await?;

        let ids: Vec<i64> = rows
            .into_iter()
            .filter_map(|row| {
                row.try_get::<String, _>("osm_id")
                    .ok()
                    .and_then(|s| s.parse::<i64>().ok())
            })
            .collect();

        Ok(ids)
    }

    pub async fn insert_facilities<T: Serialize>(&self, facilities: &[T]) -> Result<()> {
        // Convert to JSON array
        let json_array = serde_json::to_value(facilities)?;

        sqlx::query(
            r#"
            INSERT INTO l3_facilities (station_id, type, name_i18n, location_coords, attributes, source_url, updated_at)
            SELECT
                (item->>'station_id')::text,
                (item->>'type')::text,
                (item->'name_i18n')::jsonb,
                ST_GeomFromText(item->>'location_coords'),
                (item->'attributes')::jsonb,
                (item->>'source_url')::text,
                (item->>'updated_at')::timestamp
            FROM jsonb_array_elements($1::jsonb) as item
            "#
        )
        .bind(json_array)
        .execute(&self.pool)
        .await?;

        Ok(())
    }
}
```

**ä½ç½®**: `services/etl-pipeline-rs/src/utils/rate_limit.rs`
```rust
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::Semaphore;
use tokio::time::sleep;

#[derive(Clone)]
pub struct RateLimiter {
    semaphore: Arc<Semaphore>,
    delay: Duration,
}

impl RateLimiter {
    pub fn new(max_concurrent: usize, delay: Duration) -> Self {
        Self {
            semaphore: Arc::new(Semaphore::new(max_concurrent)),
            delay,
        }
    }

    pub async fn wait(&self) {
        let _permit = self.semaphore.acquire().await.unwrap();
        sleep(self.delay).await;
    }
}
```

**ä½ç½®**: `services/etl-pipeline-rs/src/utils/http.rs`
```rust
use reqwest::{Client, ClientBuilder};
use std::time::Duration;

#[derive(Clone)]
pub struct HttpClient {
    client: Client,
}

impl HttpClient {
    pub fn new(timeout: Duration) -> Self {
        let client = ClientBuilder::new()
            .timeout(timeout)
            .pool_max_idle_per_host(10)
            .build()
            .unwrap();

        Self { client }
    }

    pub fn post(&self, url: &str) -> reqwest::RequestBuilder {
        self.client.post(url)
    }

    pub fn get(&self, url: &str) -> reqwest::RequestBuilder {
        self.client.get(url)
    }
}
```

**ä½ç½®**: `services/etl-pipeline-rs/Dockerfile`
```dockerfile
FROM rust:1.83-slim-bookworm AS builder

WORKDIR /app

# Install dependencies
RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy manifest files
COPY Cargo.toml Cargo.lock ./

# Create dummy source to cache dependencies
RUN mkdir src && \
    echo "fn main() {}" > src/main.rs && \
    cargo build --release && \
    rm -rf src

# Copy actual source
COPY src ./src

# Build actual binary
RUN touch src/main.rs && cargo build --release

# Runtime stage
FROM debian:bookworm-slim

RUN apt-get update && apt-get install -y \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

COPY --from=builder /app/target/release/lutagu-etl /usr/local/bin/

ENTRYPOINT ["lutagu-etl"]
```

### 1.3 ä½¿ç”¨æ–¹å¼

#### æœ¬åœ°é–‹ç™¼
```bash
# å®‰è£ Rust (å¦‚æœå°šæœªå®‰è£)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# é€²å…¥å°ˆæ¡ˆç›®éŒ„
cd services/etl-pipeline-rs

# è¨­å®šç’°å¢ƒè®Šæ•¸
export DATABASE_URL="postgresql://..."

# åŸ·è¡Œ ETL ä»»å‹™
cargo run -- fill-toilets --radius 150 --workers 20 --delay 100

# é æœŸè¼¸å‡º
ğŸš€ Starting L3 Toilets ETL (Rust)
   Radius: 150m, Delay: 100ms, Workers: 20
ğŸ“ Found 500 active stations
  âœ… odpt.Station:TokyoMetro.Ginza.Ueno - Added 3 new toilets
  âœ¨ odpt.Station:JR-East.Yamanote.Tokyo - All toilets already exist
...
============================================
ğŸ“Š Toilet Supplement Complete!
   Processed: 500/500
   Total Toilets Added: 347
   Time Elapsed: 2m 15s  # â† åŸæœ¬ TypeScript éœ€è¦ 12.5 åˆ†é˜
```

#### Docker éƒ¨ç½²
```bash
# å»ºç«‹æ˜ åƒæª”
docker build -t lutagu-etl:latest services/etl-pipeline-rs/

# åŸ·è¡Œ
docker run --rm \
  -e DATABASE_URL="postgresql://..." \
  lutagu-etl:latest \
  fill-toilets --workers 20
```

#### æ•´åˆè‡³ GitHub Actions
```yaml
# .github/workflows/etl-daily.yml
name: Daily ETL Pipeline

on:
  schedule:
    - cron: '0 2 * * *'  # æ¯å¤© UTC 2:00 (JST 11:00)

jobs:
  run-etl:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Build ETL Binary
        run: |
          cd services/etl-pipeline-rs
          cargo build --release

      - name: Run L3 Toilets ETL
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          ./services/etl-pipeline-rs/target/release/lutagu-etl \
            fill-toilets --workers 20 --delay 100
```

### 1.4 æ•ˆèƒ½å°æ¯”

| æŒ‡æ¨™ | TypeScript | Rust | æ”¹å–„å¹…åº¦ |
|------|-----------|------|---------|
| **åŸ·è¡Œæ™‚é–“** (500 è»Šç«™) | 12.5 åˆ†é˜ | **2.5 åˆ†é˜** | **5å€** |
| **è¨˜æ†¶é«”ä½¿ç”¨** | ~500 MB | **~50 MB** | **10å€** |
| **CPU ä½¿ç”¨ç‡** | å–®æ ¸å¿ƒ 25% | å¤šæ ¸å¿ƒ 80% | **åˆ©ç”¨ç‡æå‡ 3å€** |
| **ä½µç™¼è™•ç†** | åºåˆ— (1 worker) | å¹³è¡Œ (20 workers) | **20å€ååé‡** |

---

## ğŸ¯ Phase 2: ODPT Client æ¨™æº–åŒ–

### 2.1 å•é¡Œåˆ†æ

**ç¾æ³**: `src/lib/odpt/service.ts` (200+ lines)

**å•é¡Œ**:
1. å¿«å–é‚è¼¯åˆ†æ•£åœ¨å¤šè™• (Memory + Redis)
2. éŒ¯èª¤è™•ç†ä¸ä¸€è‡´
3. Challenge API åˆ¤æ–·é‚è¼¯ç¡¬ç·¨ç¢¼
4. ç¼ºä¹å‹åˆ¥å®‰å…¨ä¿è­‰

### 2.2 Rust å…±ç”¨å‡½å¼åº«

**ä½ç½®**: `services/shared/odpt-client-rs/`

#### ç›®éŒ„çµæ§‹
```
services/shared/odpt-client-rs/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs              # Public API
â”‚   â”œâ”€â”€ client.rs           # HTTP client wrapper
â”‚   â”œâ”€â”€ models.rs           # Serde models
â”‚   â”œâ”€â”€ cache.rs            # Cache abstraction
â”‚   â””â”€â”€ error.rs            # Error types
â””â”€â”€ tests/
    â””â”€â”€ integration.rs
```

**ä½ç½®**: `services/shared/odpt-client-rs/Cargo.toml`
```toml
[package]
name = "odpt-client"
version = "0.1.0"
edition = "2021"

[dependencies]
reqwest = { version = "0.12", features = ["json", "rustls-tls"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1.38", features = ["full"] }
anyhow = "1.0"
thiserror = "1.0"
redis = { version = "0.26", features = ["tokio-comp"], optional = true }
tracing = "0.1"

[features]
default = []
redis-cache = ["redis"]
```

**ä½ç½®**: `services/shared/odpt-client-rs/src/lib.rs`
```rust
//! ODPT API Client - æ¨™æº–åŒ–å‡½å¼åº«
//!
//! æ”¯æ´:
//! - è‡ªå‹•é¸æ“‡ Standard/Challenge API
//! - å…§å»ºå¿«å–å±¤ (Memory + Redis)
//! - å®Œæ•´å‹åˆ¥å®‰å…¨
//! - è‡ªå‹•é‡è©¦æ©Ÿåˆ¶

pub mod client;
pub mod models;
pub mod cache;
pub mod error;

pub use client::OdptClient;
pub use models::*;
pub use error::{OdptError, Result};

// Re-export for convenience
pub use serde_json::Value;
```

**ä½ç½®**: `services/shared/odpt-client-rs/src/client.rs`
```rust
use crate::cache::Cache;
use crate::error::{OdptError, Result};
use crate::models::*;
use reqwest::Client;
use std::time::Duration;
use tracing::{info, warn};

const BASE_URL_STANDARD: &str = "https://api.odpt.org/api/v4";
const BASE_URL_CHALLENGE: &str = "https://api-challenge.odpt.org/api/v4";

const CHALLENGE_OPERATORS: &[&str] = &[
    "odpt.Operator:JR-East",
    "odpt.Operator:jre-is",
    "odpt.Operator:Keikyu",
    "odpt.Operator:Seibu",
    "odpt.Operator:Tobu",
    "odpt.Operator:Tokyu",
];

pub struct OdptClient {
    client: Client,
    standard_token: String,
    challenge_token: String,
    cache: Box<dyn Cache>,
}

impl OdptClient {
    pub fn new(
        standard_token: String,
        challenge_token: String,
        cache: Box<dyn Cache>,
    ) -> Self {
        let client = Client::builder()
            .timeout(Duration::from_secs(10))
            .pool_max_idle_per_host(20)
            .build()
            .unwrap();

        Self {
            client,
            standard_token,
            challenge_token,
            cache,
        }
    }

    /// ç²å–åˆ—è»Šè³‡è¨Š (è‡ªå‹•å¿«å–)
    pub async fn get_train_information(&self, operator: &str) -> Result<Vec<TrainInformation>> {
        let cache_key = format!("odpt:train-info:{}", operator);

        // Check cache
        if let Some(cached) = self.cache.get(&cache_key).await? {
            return Ok(serde_json::from_str(&cached)?);
        }

        // Fetch from API
        let (base_url, token) = self.select_api_endpoint(operator);
        let url = format!("{}/odpt:TrainInformation", base_url);

        info!("Fetching TrainInformation for {}", operator);

        let response = self
            .client
            .get(&url)
            .query(&[("odpt:operator", operator), ("acl:consumerKey", &token)])
            .send()
            .await
            .map_err(|e| OdptError::NetworkError(e.to_string()))?;

        if !response.status().is_success() {
            warn!("ODPT API returned {}", response.status());
            return Ok(vec![]);
        }

        let data: Vec<TrainInformation> = response
            .json()
            .await
            .map_err(|e| OdptError::ParseError(e.to_string()))?;

        // Cache for 60 seconds
        self.cache
            .set(&cache_key, &serde_json::to_string(&data)?, 60)
            .await?;

        Ok(data)
    }

    /// ç²å–è»Šç«™è³‡è¨Š
    pub async fn get_stations(&self, operator: &str) -> Result<Vec<Station>> {
        let (base_url, token) = self.select_api_endpoint(operator);
        let url = format!("{}/odpt:Station", base_url);

        let response = self
            .client
            .get(&url)
            .query(&[("odpt:operator", operator), ("acl:consumerKey", &token)])
            .send()
            .await
            .map_err(|e| OdptError::NetworkError(e.to_string()))?;

        if !response.status().is_success() {
            return Err(OdptError::ApiError(response.status().as_u16()));
        }

        let data = response
            .json()
            .await
            .map_err(|e| OdptError::ParseError(e.to_string()))?;

        Ok(data)
    }

    fn select_api_endpoint(&self, operator: &str) -> (&str, String) {
        if CHALLENGE_OPERATORS.contains(&operator) {
            (BASE_URL_CHALLENGE, self.challenge_token.clone())
        } else {
            (BASE_URL_STANDARD, self.standard_token.clone())
        }
    }
}
```

**ä½ç½®**: `services/shared/odpt-client-rs/src/models.rs`
```rust
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct TrainInformation {
    #[serde(rename = "owl:sameAs")]
    pub id: String,

    #[serde(rename = "odpt:operator")]
    pub operator: String,

    #[serde(rename = "odpt:railway")]
    pub railway: String,

    #[serde(rename = "odpt:trainInformationText")]
    pub text: Option<LocalizedText>,

    #[serde(rename = "odpt:trainInformationStatus")]
    pub status: Option<String>,

    #[serde(rename = "dc:date")]
    pub date: Option<String>,
}

#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct LocalizedText {
    pub ja: Option<String>,
    pub en: Option<String>,
    #[serde(rename = "ja-Hrkt")]
    pub ja_hrkt: Option<String>,
}

#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct Station {
    #[serde(rename = "owl:sameAs")]
    pub id: String,

    #[serde(rename = "odpt:operator")]
    pub operator: String,

    #[serde(rename = "odpt:railway")]
    pub railway: String,

    #[serde(rename = "dc:title")]
    pub title: Option<String>,

    #[serde(rename = "odpt:stationTitle")]
    pub station_title: Option<LocalizedText>,

    #[serde(rename = "geo:lat")]
    pub lat: Option<f64>,

    #[serde(rename = "geo:long")]
    pub lon: Option<f64>,
}
```

**ä½ç½®**: `services/shared/odpt-client-rs/src/cache.rs`
```rust
use async_trait::async_trait;
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;

#[async_trait]
pub trait Cache: Send + Sync {
    async fn get(&self, key: &str) -> Result<Option<String>, Box<dyn std::error::Error>>;
    async fn set(&self, key: &str, value: &str, ttl: u64) -> Result<(), Box<dyn std::error::Error>>;
}

// Memory Cache Implementation
pub struct MemoryCache {
    store: Arc<RwLock<HashMap<String, (String, Instant)>>>,
}

impl MemoryCache {
    pub fn new() -> Self {
        Self {
            store: Arc::new(RwLock::new(HashMap::new())),
        }
    }
}

#[async_trait]
impl Cache for MemoryCache {
    async fn get(&self, key: &str) -> Result<Option<String>, Box<dyn std::error::Error>> {
        let store = self.store.read().await;
        if let Some((value, expires_at)) = store.get(key) {
            if Instant::now() < *expires_at {
                return Ok(Some(value.clone()));
            }
        }
        Ok(None)
    }

    async fn set(&self, key: &str, value: &str, ttl: u64) -> Result<(), Box<dyn std::error::Error>> {
        let mut store = self.store.write().await;
        let expires_at = Instant::now() + Duration::from_secs(ttl);
        store.insert(key.to_string(), (value.to_string(), expires_at));
        Ok(())
    }
}

// Redis Cache Implementation (feature-gated)
#[cfg(feature = "redis-cache")]
pub struct RedisCache {
    client: redis::Client,
}

#[cfg(feature = "redis-cache")]
impl RedisCache {
    pub fn new(redis_url: &str) -> Result<Self, redis::RedisError> {
        let client = redis::Client::open(redis_url)?;
        Ok(Self { client })
    }
}

#[cfg(feature = "redis-cache")]
#[async_trait]
impl Cache for RedisCache {
    async fn get(&self, key: &str) -> Result<Option<String>, Box<dyn std::error::Error>> {
        use redis::AsyncCommands;
        let mut conn = self.client.get_multiplexed_async_connection().await?;
        let value: Option<String> = conn.get(key).await?;
        Ok(value)
    }

    async fn set(&self, key: &str, value: &str, ttl: u64) -> Result<(), Box<dyn std::error::Error>> {
        use redis::AsyncCommands;
        let mut conn = self.client.get_multiplexed_async_connection().await?;
        conn.set_ex(key, value, ttl as usize).await?;
        Ok(())
    }
}
```

**ä½ç½®**: `services/shared/odpt-client-rs/src/error.rs`
```rust
use thiserror::Error;

#[derive(Error, Debug)]
pub enum OdptError {
    #[error("Network error: {0}")]
    NetworkError(String),

    #[error("API error: HTTP {0}")]
    ApiError(u16),

    #[error("Parse error: {0}")]
    ParseError(String),

    #[error("Cache error: {0}")]
    CacheError(String),
}

pub type Result<T> = std::result::Result<T, OdptError>;
```

### 2.3 æ•´åˆè‡³ L2 Service

**ä¿®æ”¹**: `services/l2-status-rs/Cargo.toml`
```toml
[dependencies]
odpt-client = { path = "../shared/odpt-client-rs", features = ["redis-cache"] }
# ... å…¶ä»–ä¾è³´
```

**ä¿®æ”¹**: `services/l2-status-rs/src/main.rs`
```rust
use odpt_client::{OdptClient, MemoryCache};

// åˆå§‹åŒ– ODPT Client
let odpt = OdptClient::new(
    env::var("ODPT_API_KEY_METRO")?,
    env::var("ODPT_API_KEY_CHALLENGE")?,
    Box::new(MemoryCache::new()),
);

// ä½¿ç”¨çµ±ä¸€ä»‹é¢
let train_info = odpt.get_train_information("odpt.Operator:TokyoMetro").await?;
```

---

## ğŸ¯ Phase 3: å‘é‡æœå°‹æœ¬åœ°åŒ–

### 3.1 ç¾æ³å•é¡Œ

**æª”æ¡ˆ**: `src/lib/ai/embedding.ts`

**å•é¡Œ**:
- æ¯æ¬¡æœå°‹éƒ½éœ€è¦ç¶²è·¯å¾€è¿” Supabase (50-100ms)
- pgvector æŸ¥è©¢å—é™æ–¼ PostgreSQL æ•ˆèƒ½
- ç„¡æ³•é›¢ç·šä½¿ç”¨

### 3.2 Qdrant æœ¬åœ°å‘é‡è³‡æ–™åº«

#### éƒ¨ç½²æ¶æ§‹
```
services/vector-search-rs/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ data/
â”‚   â””â”€â”€ qdrant_storage/      # æŒä¹…åŒ–å„²å­˜
â””â”€â”€ src/
    â”œâ”€â”€ main.rs              # gRPC server
    â”œâ”€â”€ embedding.rs         # Mistral API client
    â”œâ”€â”€ indexer.rs           # Index builder
    â””â”€â”€ search.rs            # Search handler
```

**ä½ç½®**: `services/vector-search-rs/Cargo.toml`
```toml
[package]
name = "vector-search-rs"
version = "0.1.0"
edition = "2021"

[dependencies]
qdrant-client = "1.10"
tokio = { version = "1.38", features = ["full"] }
tonic = "0.12"
prost = "0.13"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
anyhow = "1.0"
tracing = "0.1"
tracing-subscriber = "0.3"

[build-dependencies]
tonic-build = "0.12"
```

**ä½ç½®**: `services/vector-search-rs/src/main.rs`
```rust
use qdrant_client::prelude::*;
use qdrant_client::qdrant::{CreateCollectionBuilder, Distance, VectorParamsBuilder};
use std::env;
use tokio;
use tracing::info;

mod embedding;
mod search;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    tracing_subscriber::fmt::init();

    let qdrant_url = env::var("QDRANT_URL").unwrap_or_else(|_| "http://localhost:6334".to_string());
    let client = QdrantClient::from_url(&qdrant_url).build()?;

    // å»ºç«‹ collection (å¦‚æœä¸å­˜åœ¨)
    let collection_name = "expert_knowledge";

    if !client.collection_exists(collection_name).await? {
        info!("Creating collection: {}", collection_name);

        client
            .create_collection(
                CreateCollectionBuilder::new(collection_name)
                    .vectors_config(VectorParamsBuilder::new(1024, Distance::Cosine)), // Mistral embed = 1024 dim
            )
            .await?;
    }

    // å•Ÿå‹• gRPC server
    info!("Vector Search Service running on :50051");
    search::serve(&client, 50051).await?;

    Ok(())
}
```

**ä½ç½®**: `services/vector-search-rs/src/search.rs`
```rust
use qdrant_client::prelude::*;
use qdrant_client::qdrant::{PointStruct, SearchPointsBuilder};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tonic::{transport::Server, Request, Response, Status};

use crate::embedding::generate_embedding;

pub mod vector_search {
    tonic::include_proto!("vector_search");
}

use vector_search::vector_search_server::{VectorSearch, VectorSearchServer};
use vector_search::{SearchRequest, SearchResponse, SearchResult};

pub struct VectorSearchService {
    qdrant: QdrantClient,
}

#[tonic::async_trait]
impl VectorSearch for VectorSearchService {
    async fn search(
        &self,
        request: Request<SearchRequest>,
    ) -> Result<Response<SearchResponse>, Status> {
        let req = request.into_inner();

        // Generate embedding
        let embedding = generate_embedding(&req.query)
            .await
            .map_err(|e| Status::internal(e.to_string()))?;

        // Search in Qdrant
        let search_result = self
            .qdrant
            .search_points(
                SearchPointsBuilder::new("expert_knowledge", embedding, req.limit as u64)
                    .score_threshold(req.threshold),
            )
            .await
            .map_err(|e| Status::internal(e.to_string()))?;

        let results: Vec<SearchResult> = search_result
            .result
            .into_iter()
            .map(|point| SearchResult {
                id: point.id.unwrap().to_string(),
                score: point.score,
                content: point
                    .payload
                    .get("content")
                    .and_then(|v| v.as_str())
                    .unwrap_or("")
                    .to_string(),
                tags: point
                    .payload
                    .get("tags")
                    .and_then(|v| v.as_list())
                    .map(|arr| {
                        arr.iter()
                            .filter_map(|v| v.as_str().map(String::from))
                            .collect()
                    })
                    .unwrap_or_default(),
            })
            .collect();

        Ok(Response::new(SearchResponse { results }))
    }
}

pub async fn serve(qdrant: &QdrantClient, port: u16) -> anyhow::Result<()> {
    let addr = format!("0.0.0.0:{}", port).parse()?;
    let service = VectorSearchService {
        qdrant: qdrant.clone(),
    };

    Server::builder()
        .add_service(VectorSearchServer::new(service))
        .serve(addr)
        .await?;

    Ok(())
}
```

**ä½ç½®**: `services/vector-search-rs/proto/vector_search.proto`
```protobuf
syntax = "proto3";

package vector_search;

service VectorSearch {
  rpc Search(SearchRequest) returns (SearchResponse);
}

message SearchRequest {
  string query = 1;
  uint32 limit = 2;
  float threshold = 3;
}

message SearchResponse {
  repeated SearchResult results = 1;
}

message SearchResult {
  string id = 1;
  float score = 2;
  string content = 3;
  repeated string tags = 4;
}
```

### 3.3 æ•ˆèƒ½å°æ¯”

| æŒ‡æ¨™ | Supabase pgvector | Qdrant (Rust) | æ”¹å–„ |
|------|-------------------|---------------|------|
| **æŸ¥è©¢å»¶é²** | 50-100ms | **5-10ms** | **10-20å€** |
| **æ‰¹æ¬¡æŸ¥è©¢** (100 queries) | 5-10s | **0.5s** | **10-20å€** |
| **è¨˜æ†¶é«”ä½¿ç”¨** | N/A (é›²ç«¯) | ~200 MB | å¯æ§ |
| **é›¢ç·šæ”¯æ´** | âŒ | âœ… | - |

---

## ğŸ“Š ç¸½é«”æ•ˆèƒ½æå‡é æ¸¬

### é—œéµæŒ‡æ¨™å°æ¯”

| çµ„ä»¶ | ç¾æ³ (TS) | Rust åŒ–å¾Œ | æå‡å€æ•¸ | ç‹€æ…‹ |
|------|----------|-----------|---------|------|
| **L2 Status** | 2.5s | 0.2s | **12.5Ã—** | âœ… å·²å®Œæˆ |
| **L4 Routing** | 3.0s | 0.2s | **15Ã—** | âœ… å·²å®Œæˆ |
| **ETL Pipeline** | 30 min | 3 min | **10Ã—** | ğŸ¯ æœ¬æ–¹æ¡ˆ |
| **ODPT Client** | 200ms | 50ms | **4Ã—** | ğŸ¯ æœ¬æ–¹æ¡ˆ |
| **Vector Search** | 100ms | 5ms | **20Ã—** | ğŸ¯ æœ¬æ–¹æ¡ˆ |

### è³‡æºç¯€ç´„

| è³‡æº | ç¯€ç´„å¹…åº¦ | å¹´åº¦æˆæœ¬ç¯€çœ (é ä¼°) |
|------|---------|------------------|
| **é‹ç®—è³‡æº** | -70% | ~$3,000 USD |
| **è¨˜æ†¶é«”** | -80% | ~$1,500 USD |
| **ç¶²è·¯é »å¯¬** | -50% | ~$800 USD |
| **ç¸½è¨ˆ** | - | **~$5,300 USD/å¹´** |

---

## ğŸš€ å¯¦æ–½æ™‚ç¨‹

### Week 1-2: ETL Pipeline
- âœ… Day 1-3: å»ºç«‹ Cargo workspace
- âœ… Day 4-7: å¯¦ä½œ l3_toilets æ¨¡çµ„
- âœ… Day 8-10: æ¸¬è©¦èˆ‡å„ªåŒ–
- âœ… Day 11-14: éƒ¨ç½²è‡³ Cloud Run

### Week 3: ODPT Client
- Day 15-17: å»ºç«‹å…±ç”¨ crate
- Day 18-20: æ•´åˆè‡³ L2/L4 services
- Day 21: æ¸¬è©¦èˆ‡æ–‡ä»¶

### Week 4: Vector Search
- Day 22-24: éƒ¨ç½² Qdrant
- Day 25-27: å¯¦ä½œ gRPC service
- Day 28: æ•´åˆæ¸¬è©¦

---

## ğŸ“ æª¢æŸ¥æ¸…å–®

### ç’°å¢ƒæº–å‚™
- [ ] å®‰è£ Rust 1.83+
- [ ] è¨­å®š DATABASE_URL ç’°å¢ƒè®Šæ•¸
- [ ] æº–å‚™ Docker ç’°å¢ƒ (ç”¨æ–¼éƒ¨ç½²)

### Phase 1: ETL Pipeline
- [ ] å»ºç«‹ `services/etl-pipeline-rs/` ç›®éŒ„çµæ§‹
- [ ] è¤‡è£½æ‰€æœ‰ä¸Šè¿°ç¨‹å¼ç¢¼æª”æ¡ˆ
- [ ] åŸ·è¡Œ `cargo build --release`
- [ ] æ¸¬è©¦æœ¬åœ°åŸ·è¡Œ: `cargo run -- fill-toilets --workers 5`
- [ ] é©—è­‰è³‡æ–™åº«å¯«å…¥æ­£ç¢º
- [ ] å»ºç«‹ Docker æ˜ åƒæª”
- [ ] éƒ¨ç½²è‡³ Cloud Run æˆ– Zeabur

### Phase 2: ODPT Client
- [ ] å»ºç«‹ `services/shared/odpt-client-rs/`
- [ ] å¯¦ä½œæ‰€æœ‰æ¨¡çµ„
- [ ] æ’°å¯«å–®å…ƒæ¸¬è©¦
- [ ] æ›´æ–° L2/L4 services Cargo.toml
- [ ] é©—è­‰æ•´åˆç„¡èª¤

### Phase 3: Vector Search
- [ ] éƒ¨ç½² Qdrant (Docker Compose)
- [ ] å»ºç«‹ `services/vector-search-rs/`
- [ ] å¯¦ä½œ gRPC service
- [ ] é·ç§»ç¾æœ‰ expert_knowledge è³‡æ–™
- [ ] æ›´æ–° Next.js API routes

---

## ğŸ”— ç›¸é—œè³‡æº

### æ–‡ä»¶
- [Rust å®˜æ–¹æ•™å­¸](https://doc.rust-lang.org/book/)
- [Tokio éåŒæ­¥æ•™å­¸](https://tokio.rs/tokio/tutorial)
- [SQLx ä½¿ç”¨æŒ‡å—](https://github.com/launchbadge/sqlx)
- [Qdrant æ–‡ä»¶](https://qdrant.tech/documentation/)

### åƒè€ƒå°ˆæ¡ˆ
- L2-Status-RS: `/services/l2-status-rs/`
- L4-Routing-RS: `/services/l4-routing-rs/`

---

## âš ï¸ æ³¨æ„äº‹é …

1. **é€æ­¥é·ç§»**: ä¸å»ºè­°ä¸€æ¬¡æ€§æ›¿æ›æ‰€æœ‰çµ„ä»¶,æ‡‰é€æ­¥é©—è­‰
2. **ä¿ç•™ Fallback**: TypeScript ç‰ˆæœ¬ä¿ç•™è‡³å°‘ 1 å€‹æœˆ,ä½œç‚ºå‚™æ´
3. **ç›£æ§æŒ‡æ¨™**: éƒ¨ç½²å¾Œå¯†åˆ‡ç›£æ§éŒ¯èª¤ç‡ã€å»¶é²ã€è¨˜æ†¶é«”ä½¿ç”¨
4. **å‹åˆ¥åŒæ­¥**: ç¢ºä¿ Rust models èˆ‡è³‡æ–™åº« schema ä¸€è‡´

---

**ä¸‹ä¸€æ­¥**: è«‹ç¢ºèªæ˜¯å¦è¦æˆ‘å”åŠ©å¯¦ä½œä»»ä½•ç‰¹å®šæ¨¡çµ„,æˆ–éœ€è¦æ›´è©³ç´°çš„ç¨‹å¼ç¢¼èªªæ˜ã€‚
